{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2ff92ea7e6f46b",
   "metadata": {},
   "source": [
    "Q2\n",
    "Build testdataset  to verify the effectiveness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T09:27:24.693129900Z",
     "start_time": "2023-12-05T09:27:24.657129600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  NewsID                                        NewsContent  \\\n",
      "0           0       1  本报记者 田雨 李京华    　　建行原董事长张恩照受贿案３日一审宣判，北京市第一中级人民法...   \n",
      "1           1       2  中国农业银行信用卡中心由北京搬到上海了！  　　农行行长杨明生日前在信用卡中心揭牌仪式上表示...   \n",
      "2           2       3  在新基金快速发行以及申购资金回流的情况下，市场总体上呈现资金流动性过剩格局，考虑到现阶段权重...   \n",
      "3           3       4  胜利股份（000407）公司子公司填海造地2800亩，以青岛的地价估算，静态价值在10亿元左...   \n",
      "4           4       5  全景网11月30日讯 外围股市造好，带动港股今早造好，恒指高开后反覆上升，最高升252点，曾...   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"test_data.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0534f39acb58",
   "metadata": {},
   "source": [
    "## nlp_structbert_sentiment-classification_chinese-large(Selected)\n",
    "(https://modelscope.cn/models/damo/nlp_structbert_sentiment-classification_chinese-large/summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "922a96de64adf58f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T05:26:34.729437900Z",
     "start_time": "2023-12-04T05:26:30.393768400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 13:26:31,847 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2023-12-04 13:26:32,237 - modelscope - INFO - initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_structbert_sentiment-classification_chinese-large\n",
      "2023-12-04 13:26:32,238 - modelscope - INFO - initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_structbert_sentiment-classification_chinese-large.\n",
      "2023-12-04 13:26:32,243 - modelscope - INFO - initialize model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_structbert_sentiment-classification_chinese-large\n",
      "2023-12-04 13:26:34,073 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "2023-12-04 13:26:34,413 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTextClassification.\n",
      "\n",
      "2023-12-04 13:26:34,414 - modelscope - INFO - All the weights of ModelForTextClassification were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTextClassification for predictions without further training.\n",
      "2023-12-04 13:26:34,424 - modelscope - INFO - The key of sentence1: sentence1, The key of sentence2: None, The key of label: label\n",
      "2023-12-04 13:26:34,435 - modelscope - INFO - The key of sentence1: text, The key of sentence2: None, The key of label: label\n",
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.580305278301239, 0.419694721698761], 'labels': ['正面', '负面']}\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "semantic_cls = pipeline(Tasks.text_classification, 'damo/nlp_structbert_sentiment-classification_chinese-large')\n",
    "result = semantic_cls(input='本报记者 田雨 李京华    　　中国建设银行股份有限公司原董事长张恩照受贿案３日一审宣判，北京市第一中级人民法院依法以受贿罪判处张恩照有期徒刑１５年。    　　法院经开庭审理查明，２０００年至２００４年期间，被告人张恩照利用其担任原中国建设银行副行长、行长，中国建设银行股份有限公司董事长的职务便利，为他人牟取利益，多次非法收受他人给予的款物共计人民币４００余万元。案发后，赃款、赃物已全部退缴。    　　法院认为，被告人张恩照身为国家工作人员，利用职务上的便利，为他人谋取利益，非法收受他人财物，其行为已构成受贿罪，受贿数额特别巨大。鉴于张恩照因其他违纪问题被审查后，主动交代了有关部门不掌握的本案受贿事实，属于自首，且赃款、赃物已全部退缴，对张恩照依法可从轻处罚。法院遂依法以受贿罪判处张恩照有期徒刑１５年。')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d55ff236a397c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T05:26:50.666578400Z",
     "start_time": "2023-12-04T05:26:34.728437600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8865435356200527\n"
     ]
    }
   ],
   "source": [
    "def classify_sentiment(text):\n",
    "    result = semantic_cls(input=text)\n",
    "    if 'scores' in result and 'labels' in result:\n",
    "        scores = result['scores']\n",
    "        labels = result['labels']\n",
    "        if labels[0] == '正面':\n",
    "            positive_score = scores[0]\n",
    "            return 1 if positive_score > 0.5 else 0\n",
    "        else:\n",
    "            negative_score = scores[0]\n",
    "            return 0 if negative_score > 0.5 else 1\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "df['predicted_label'] = df['NewsContent'].apply(classify_sentiment)\n",
    "accuracy = (df['predicted_label'] == df['label']).mean()\n",
    "print(f\"accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e6511d67afc457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:40:40.848797200Z",
     "start_time": "2023-12-01T16:40:40.817792500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NewsID</th>\n",
       "      <th>NewsContent</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>本报记者 田雨 李京华    　　建行原董事长张恩照受贿案３日一审宣判，北京市第一中级人民法...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>中国农业银行信用卡中心由北京搬到上海了！  　　农行行长杨明生日前在信用卡中心揭牌仪式上表示...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>在新基金快速发行以及申购资金回流的情况下，市场总体上呈现资金流动性过剩格局，考虑到现阶段权重...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>胜利股份（000407）公司子公司填海造地2800亩，以青岛的地价估算，静态价值在10亿元左...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>全景网11月30日讯 外围股市造好，带动港股今早造好，恒指高开后反覆上升，最高升252点，曾...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>500097</td>\n",
       "      <td>证券时报e公司讯，昆仑万维(300418)12月4日晚间公告，公司决定在原有投资业务板块基础...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>500098</td>\n",
       "      <td>温氏股份公告，公司拟以集中竞价交易方式回购股份，回购金额不低于9亿元，不超18亿元；回购价格...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>500099</td>\n",
       "      <td>普利特公告，控股子公司江苏海四达电源有限公司将投资建设年产1.3GWh钠离子及锂离子电池数字...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>500100</td>\n",
       "      <td>普利制药12月4日公告，公司于近日收到了奥地利联邦卫生安全办公室签发的注射用更昔洛韦的上市许...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>500101</td>\n",
       "      <td>上机数控公告，公司全资子公司弘元新材料(徐州)有限公司位于徐州经开区金凤路西侧一项目工地发生...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>379 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NewsID                                        NewsContent  label  \\\n",
       "0        1  本报记者 田雨 李京华    　　建行原董事长张恩照受贿案３日一审宣判，北京市第一中级人民法...      0   \n",
       "1        2  中国农业银行信用卡中心由北京搬到上海了！  　　农行行长杨明生日前在信用卡中心揭牌仪式上表示...      1   \n",
       "2        3  在新基金快速发行以及申购资金回流的情况下，市场总体上呈现资金流动性过剩格局，考虑到现阶段权重...      1   \n",
       "3        4  胜利股份（000407）公司子公司填海造地2800亩，以青岛的地价估算，静态价值在10亿元左...      1   \n",
       "4        5  全景网11月30日讯 外围股市造好，带动港股今早造好，恒指高开后反覆上升，最高升252点，曾...      1   \n",
       "..     ...                                                ...    ...   \n",
       "75  500097  证券时报e公司讯，昆仑万维(300418)12月4日晚间公告，公司决定在原有投资业务板块基础...      1   \n",
       "76  500098  温氏股份公告，公司拟以集中竞价交易方式回购股份，回购金额不低于9亿元，不超18亿元；回购价格...      1   \n",
       "77  500099  普利特公告，控股子公司江苏海四达电源有限公司将投资建设年产1.3GWh钠离子及锂离子电池数字...      1   \n",
       "78  500100  普利制药12月4日公告，公司于近日收到了奥地利联邦卫生安全办公室签发的注射用更昔洛韦的上市许...      1   \n",
       "79  500101  上机数控公告，公司全资子公司弘元新材料(徐州)有限公司位于徐州经开区金凤路西侧一项目工地发生...      1   \n",
       "\n",
       "    predicted_label  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "..              ...  \n",
       "75                1  \n",
       "76                1  \n",
       "77                1  \n",
       "78                1  \n",
       "79                0  \n",
       "\n",
       "[379 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e4c82ee77aedd",
   "metadata": {},
   "source": [
    "## Erlangshen-RoBERTa-330M-Sentiment\n",
    "(https://modelscope.cn/models/Fengshenbang/Erlangshen-RoBERTa-330M-Sentiment/summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261d58d540dfe17e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:40:44.819284300Z",
     "start_time": "2023-12-01T16:40:40.834792800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 00:40:42,122 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2023-12-02 00:40:42,398 - modelscope - INFO - initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\Fengshenbang\\Erlangshen-RoBERTa-330M-Sentiment\n",
      "2023-12-02 00:40:42,398 - modelscope - INFO - initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\Fengshenbang\\Erlangshen-RoBERTa-330M-Sentiment.\n",
      "2023-12-02 00:40:42,408 - modelscope - INFO - initialize model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\Fengshenbang\\Erlangshen-RoBERTa-330M-Sentiment\n",
      "2023-12-02 00:40:44,214 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "2023-12-02 00:40:44,779 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTextClassification.\n",
      "\n",
      "2023-12-02 00:40:44,780 - modelscope - INFO - All the weights of ModelForTextClassification were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTextClassification for predictions without further training.\n",
      "2023-12-02 00:40:44,790 - modelscope - INFO - The key of sentence1: sentence1, The key of sentence2: None, The key of label: label\n",
      "2023-12-02 00:40:44,800 - modelscope - INFO - The key of sentence1: text, The key of sentence2: None, The key of label: label\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "p = pipeline(Tasks.text_classification, 'Fengshenbang/Erlangshen-RoBERTa-330M-Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76da9b960a327aa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:41:01.321002200Z",
     "start_time": "2023-12-01T16:40:44.816286200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8575197889182058\n"
     ]
    }
   ],
   "source": [
    "def classify_sentiment2(text):\n",
    "    result = p(input=text)\n",
    "    if 'scores' in result and 'labels' in result:\n",
    "        scores = result['scores']\n",
    "        labels = result['labels']\n",
    "        if labels[0] == '正向':\n",
    "            positive_score = scores[0]\n",
    "            return 1 if positive_score > 0.5 else 0\n",
    "        else:\n",
    "            negative_score = scores[0]\n",
    "            return 0 if negative_score > 0.5 else 1\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "\n",
    "df['predicted_label2'] = df['NewsContent'].apply(classify_sentiment2)\n",
    "\n",
    "\n",
    "accuracy = (df['predicted_label2'] == df['label']).mean()\n",
    "print(f\"accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a3c1ef4993020",
   "metadata": {},
   "source": [
    "Erlangshen-MegatronBert-1.3B-Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a52e9f010128c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:53:01.261215200Z",
     "start_time": "2023-12-01T16:41:01.321002200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87aabe56188e47d28f52482b37104df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902354c845564ba786a1de2d26e74830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b73fec4277544c398e7a9b0b4f0e457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9961, 0.0039]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-Sentiment')\n",
    "model=AutoModelForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-Sentiment')\n",
    "\n",
    "text='今天心情不好'\n",
    "output=model(torch.tensor([tokenizer.encode(text)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1106161627d0ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T17:06:04.399149900Z",
     "start_time": "2023-12-01T16:54:12.779891800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8601583113456465\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-Sentiment',max_length=1512)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-MegatronBert-1.3B-Sentiment')\n",
    "\n",
    "def classify_sentiment(text, max_length=512):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
    "    with torch.no_grad():  \n",
    "        output = model(**encoded_input)\n",
    "    probabilities = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "    negative, positive = probabilities.detach().numpy()[0]\n",
    "    return 1 if positive > negative else 0\n",
    "\n",
    "df['predicted_label'] = df['NewsContent'].apply(classify_sentiment)\n",
    "\n",
    "accuracy = (df['predicted_label'] == df['label']).mean()\n",
    "print(f\"accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481bfe113f4b0077",
   "metadata": {},
   "source": [
    "## bardsai/finance-sentiment-zh-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a81381ed4284fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:53:03.132521400Z",
     "start_time": "2023-12-01T16:53:01.261215200Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=\"bardsai/finance-sentiment-zh-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a470eb855641ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:54:05.144816500Z",
     "start_time": "2023-12-01T16:53:03.134521800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.46701846965699206\n"
     ]
    }
   ],
   "source": [
    "def classify_sentiment(text, max_length=512):\n",
    "    parts = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    scores = []\n",
    "    \n",
    "    for part in parts:\n",
    "        result = nlp(part)\n",
    "        if result and isinstance(result, list) and 'label' in result[0]:\n",
    "            label = result[0]['label']\n",
    "            scores.append(1 if label == 'positive' else 0)\n",
    "\n",
    "    if scores:\n",
    "        return round(sum(scores) / len(scores))\n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "\n",
    "df['predicted_label'] = df['NewsContent'].apply(classify_sentiment)\n",
    "\n",
    "\n",
    "accuracy = (df['predicted_label'] == df['label']).mean()\n",
    "print(f\"accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd6470ba6676bb",
   "metadata": {},
   "source": [
    "### germla/satoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18364915d9c5c322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:54:07.847465600Z",
     "start_time": "2023-12-01T16:54:05.144816500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "# Download from Hub and run inference\n",
    "model = SetFitModel.from_pretrained(\"germla/satoken\")\n",
    "# Run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c036472fc1ec7f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:54:12.777891400Z",
     "start_time": "2023-12-01T16:54:07.850466400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8284960422163589\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "\n",
    "model = SetFitModel.from_pretrained(\"germla/satoken\")\n",
    "\n",
    "def classify_sentiment(text):\n",
    "\n",
    "    prediction = model([text])\n",
    "    return prediction[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['predicted_label'] = df['NewsContent'].apply(classify_sentiment)\n",
    "\n",
    "accuracy = (df['predicted_label'] == df['label']).mean()\n",
    "print(f\"accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384771c87a92e4e7",
   "metadata": {},
   "source": [
    "Select the StructBERT Chinese large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28673935d783bee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T06:35:52.904705600Z",
     "start_time": "2023-12-04T06:35:22.931803600Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('Task1Q1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90274ba026e4a8fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T06:35:52.920706Z",
     "start_time": "2023-12-04T06:35:52.905706200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         NewsID                                        NewsContent  \\\n0             1  本报记者 田雨 李京华    　　中国建设银行股份有限公司原董事长张恩照受贿案３日一审宣判，...   \n1             2  中国农业银行信用卡中心由北京搬到上海了！  　　农行行长杨明生日前在信用卡中心揭牌仪式上表示...   \n2             3  在新基金快速发行以及申购资金回流的情况下，市场总体上呈现资金流动性过剩格局，考虑到现阶段权重...   \n3             4  胜利股份（000407）公司子公司填海造地2800亩，以青岛的地价估算，静态价值在10亿元左...   \n4             5  全景网11月30日讯 外围股市造好，带动港股今早造好，恒指高开后反覆上升，最高升252点，曾...   \n...         ...                                                ...   \n489884  1037031  每经AI快讯，有投资者在投资者互动平台提问：请问公司目前有没有电解槽产能，规划情况能否详细介...   \n489885  1037032  依米康（SZ 300249，收盘价：10.38元）发布公告称，2023年10月12日，依米康...   \n489886  1037033  天风证券10月13日发布研报称，给予中核科技（000777.SZ，最新价：13.03元）买入...   \n489887  1037034  有投资者提问：抗癌药CPT获批后，公司是否应该按照股权协议继续收购沙东股权，适应症为MM的C...   \n489888  1037035  10月13日午间，根据恩捷股份发布的公告，持有公司股份5%以上的股东玉溪合益投资有限公司（下...   \n\n       Explicit_Company  \n0                  建设银行  \n1                  农业银行  \n2            中国国航, 外运发展  \n3                  胜利股份  \n4                  中国银行  \n...                 ...  \n489884              亿华通  \n489885        中泰证券, 依米康  \n489886       天风证券, 中核科技  \n489887             海特生物  \n489888             恩捷股份  \n\n[489889 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NewsID</th>\n      <th>NewsContent</th>\n      <th>Explicit_Company</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>本报记者 田雨 李京华    　　中国建设银行股份有限公司原董事长张恩照受贿案３日一审宣判，...</td>\n      <td>建设银行</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>中国农业银行信用卡中心由北京搬到上海了！  　　农行行长杨明生日前在信用卡中心揭牌仪式上表示...</td>\n      <td>农业银行</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>在新基金快速发行以及申购资金回流的情况下，市场总体上呈现资金流动性过剩格局，考虑到现阶段权重...</td>\n      <td>中国国航, 外运发展</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>胜利股份（000407）公司子公司填海造地2800亩，以青岛的地价估算，静态价值在10亿元左...</td>\n      <td>胜利股份</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>全景网11月30日讯 外围股市造好，带动港股今早造好，恒指高开后反覆上升，最高升252点，曾...</td>\n      <td>中国银行</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>489884</th>\n      <td>1037031</td>\n      <td>每经AI快讯，有投资者在投资者互动平台提问：请问公司目前有没有电解槽产能，规划情况能否详细介...</td>\n      <td>亿华通</td>\n    </tr>\n    <tr>\n      <th>489885</th>\n      <td>1037032</td>\n      <td>依米康（SZ 300249，收盘价：10.38元）发布公告称，2023年10月12日，依米康...</td>\n      <td>中泰证券, 依米康</td>\n    </tr>\n    <tr>\n      <th>489886</th>\n      <td>1037033</td>\n      <td>天风证券10月13日发布研报称，给予中核科技（000777.SZ，最新价：13.03元）买入...</td>\n      <td>天风证券, 中核科技</td>\n    </tr>\n    <tr>\n      <th>489887</th>\n      <td>1037034</td>\n      <td>有投资者提问：抗癌药CPT获批后，公司是否应该按照股权协议继续收购沙东股权，适应症为MM的C...</td>\n      <td>海特生物</td>\n    </tr>\n    <tr>\n      <th>489888</th>\n      <td>1037035</td>\n      <td>10月13日午间，根据恩捷股份发布的公告，持有公司股份5%以上的股东玉溪合益投资有限公司（下...</td>\n      <td>恩捷股份</td>\n    </tr>\n  </tbody>\n</table>\n<p>489889 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5e627cbd2a28f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T06:36:40.212989500Z",
     "start_time": "2023-12-04T06:36:27.996697700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 14:36:31,104 - modelscope - INFO - PyTorch version 2.1.1+cu118 Found.\n",
      "2023-12-04 14:36:31,108 - modelscope - INFO - Loading ast index from C:\\Users\\Administrator\\.cache\\modelscope\\ast_indexer\n",
      "2023-12-04 14:36:31,223 - modelscope - INFO - Loading done! Current index file version is 1.9.5, with md5 9d857e0795de1892e211da812c513b75 and a total number of 945 components indexed\n",
      "2023-12-04 14:36:34,516 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "2023-12-04 14:36:34,781 - modelscope - INFO - initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_structbert_sentiment-classification_chinese-large\n",
      "2023-12-04 14:36:34,782 - modelscope - INFO - initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_structbert_sentiment-classification_chinese-large.\n",
      "2023-12-04 14:36:34,786 - modelscope - INFO - initialize model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_structbert_sentiment-classification_chinese-large\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 14:36:39,391 - modelscope - INFO - head has no _keys_to_ignore_on_load_missing\n",
      "2023-12-04 14:36:40,167 - modelscope - INFO - All model checkpoint weights were used when initializing ModelForTextClassification.\n",
      "\n",
      "2023-12-04 14:36:40,167 - modelscope - INFO - All the weights of ModelForTextClassification were initialized from the model checkpoint If your task is similar to the task the model of the checkpoint was trained on, you can already use ModelForTextClassification for predictions without further training.\n",
      "2023-12-04 14:36:40,186 - modelscope - INFO - The key of sentence1: sentence1, The key of sentence2: None, The key of label: label\n",
      "2023-12-04 14:36:40,198 - modelscope - INFO - The key of sentence1: text, The key of sentence2: None, The key of label: label\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "semantic_cls = pipeline(Tasks.text_classification, 'damo/nlp_structbert_sentiment-classification_chinese-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1475f18c4a15fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T06:36:44.818021300Z",
     "start_time": "2023-12-04T06:36:40.211990100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/489889 [00:00<?, ?it/s]\u001B[AD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|          | 1/489889 [00:00<66:05:03,  2.06it/s]\u001B[A\n",
      "  0%|          | 4/489889 [00:00<17:27:12,  7.80it/s]\u001B[A\n",
      "  0%|          | 7/489889 [00:00<11:21:23, 11.98it/s]\u001B[A\n",
      "  0%|          | 10/489889 [00:00<9:03:15, 15.03it/s]\u001B[A\n",
      "  0%|          | 13/489889 [00:01<7:51:24, 17.32it/s]\u001B[A\n",
      "  0%|          | 16/489889 [00:01<7:09:04, 19.03it/s]\u001B[A\n",
      "  0%|          | 19/489889 [00:01<6:43:30, 20.23it/s]\u001B[A\n",
      "  0%|          | 22/489889 [00:01<6:32:03, 20.82it/s]\u001B[A\n",
      "  0%|          | 25/489889 [00:01<6:24:41, 21.22it/s]\u001B[A\n",
      "  0%|          | 28/489889 [00:01<6:24:54, 21.21it/s]\u001B[A\n",
      "  0%|          | 31/489889 [00:01<6:12:23, 21.92it/s]\u001B[A\n",
      "  0%|          | 34/489889 [00:01<6:07:07, 22.24it/s]\u001B[A\n",
      "  0%|          | 37/489889 [00:02<6:06:01, 22.30it/s]\u001B[A\n",
      "  0%|          | 40/489889 [00:02<6:03:33, 22.46it/s]\u001B[A\n",
      "  0%|          | 43/489889 [00:02<6:01:01, 22.61it/s]\u001B[A\n",
      "  0%|          | 46/489889 [00:02<5:57:13, 22.85it/s]\u001B[A\n",
      "  0%|          | 49/489889 [00:02<5:55:24, 22.97it/s]\u001B[A\n",
      "  0%|          | 52/489889 [00:02<5:53:37, 23.09it/s]\u001B[A\n",
      "  0%|          | 55/489889 [00:02<6:05:00, 22.37it/s]\u001B[A\n",
      "  0%|          | 58/489889 [00:03<6:01:14, 22.60it/s]\u001B[A\n",
      "  0%|          | 61/489889 [00:03<5:58:35, 22.77it/s]\u001B[A\n",
      "  0%|          | 64/489889 [00:03<5:54:41, 23.02it/s]\u001B[A\n",
      "  0%|          | 67/489889 [00:03<5:54:49, 23.01it/s]\u001B[A\n",
      "  0%|          | 70/489889 [00:03<5:54:06, 23.05it/s]\u001B[A\n",
      "  0%|          | 73/489889 [00:03<5:53:36, 23.09it/s]\u001B[A\n",
      "  0%|          | 76/489889 [00:03<5:57:44, 22.82it/s]\u001B[A\n",
      "  0%|          | 79/489889 [00:03<5:58:35, 22.77it/s]\u001B[A\n",
      "  0%|          | 82/489889 [00:04<5:55:06, 22.99it/s]\u001B[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 19\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     15\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \n\u001B[1;32m---> 19\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m [classify_sentiment(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m tqdm(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNewsContent\u001B[39m\u001B[38;5;124m'\u001B[39m])]\n",
      "Cell \u001B[1;32mIn[10], line 19\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     15\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \n\u001B[1;32m---> 19\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m [\u001B[43mclassify_sentiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m tqdm(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNewsContent\u001B[39m\u001B[38;5;124m'\u001B[39m])]\n",
      "Cell \u001B[1;32mIn[10], line 4\u001B[0m, in \u001B[0;36mclassify_sentiment\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclassify_sentiment\u001B[39m(text):\n\u001B[1;32m----> 4\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43msemantic_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscores\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m result \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m result:\n\u001B[0;32m      6\u001B[0m         scores \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscores\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\modelscope\\pipelines\\base.py:219\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[1;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_iterator(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 219\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_single\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\modelscope\\pipelines\\base.py:254\u001B[0m, in \u001B[0;36mPipeline._process_single\u001B[1;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[0;32m    252\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_auto_collate:\n\u001B[0;32m    253\u001B[0m             out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_collate_fn(out)\n\u001B[1;32m--> 254\u001B[0m         out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    256\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(out, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\modelscope\\pipelines\\nlp\\text_classification_pipeline.py:104\u001B[0m, in \u001B[0;36mTextClassificationPipeline.forward\u001B[1;34m(self, inputs, **forward_params)\u001B[0m\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    103\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mforward(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[1;32m--> 104\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\modelscope\\models\\base\\base_torch_model.py:36\u001B[0m, in \u001B[0;36mTorchModel.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(args[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 36\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\modelscope\\models\\nlp\\task_models\\task_model.py:668\u001B[0m, in \u001B[0;36mEncoderModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001B[0m\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m OutputKeys\u001B[38;5;241m.\u001B[39mLABEL \u001B[38;5;129;01min\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    667\u001B[0m     labels \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(OutputKeys\u001B[38;5;241m.\u001B[39mLABEL, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m--> 668\u001B[0m feature \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_feature\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    680\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead\u001B[38;5;241m.\u001B[39mforward(feature, attention_mask, labels, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\modelscope\\models\\nlp\\task_models\\task_model.py:586\u001B[0m, in \u001B[0;36mEncoderModel.extract_feature\u001B[1;34m(self, **input)\u001B[0m\n\u001B[0;32m    584\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    585\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 586\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    587\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    result = semantic_cls(input=text)\n",
    "    if 'scores' in result and 'labels' in result:\n",
    "        scores = result['scores']\n",
    "        labels = result['labels']\n",
    "        if labels[0] == '正面':\n",
    "            positive_score = scores[0]\n",
    "            return 1 if positive_score > 0.5 else 0\n",
    "        else:\n",
    "            negative_score = scores[0]\n",
    "            return 0 if negative_score > 0.5 else 1\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "\n",
    "\n",
    "df['label'] = [classify_sentiment(text) for text in tqdm(df['NewsContent'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591147a50b33c61",
   "metadata": {},
   "source": [
    "dataset used to finetune :sara-nabhani/ML-news-sentiment MoritzLaurer/sentiment_economy_news  FinanceInc/auditor_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac5ebf4d12c4988d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T09:11:34.963515400Z",
     "start_time": "2023-11-24T09:11:34.947515700Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# import os.path as osp\n",
    "# from modelscope.trainers import build_trainer\n",
    "# from modelscope.msdatasets import MsDataset\n",
    "# from modelscope.utils.hub import read_config\n",
    "# from modelscope.metainfo import Metrics\n",
    "# \n",
    "# # df = df[['NewsContent', 'label']]\n",
    "# # \n",
    "# 本\n",
    "# # label_map = {0: '负面', 1: '正面'}\n",
    "# # df['label'] = df['label'].map(label_map)\n",
    "# # \n",
    "# # # Rename the 'NewsContent' column to 'sentence'\n",
    "# # df.to_csv('train.csv')\n",
    "# model_id = 'damo/nlp_structbert_sentiment-classification_chinese-large'\n",
    "# \n",
    "# \n",
    "# WORK_DIR = 'workspace'\n",
    "# max_epochs=1\n",
    "# from modelscope.msdatasets import MsDataset\n",
    "# train_dataset =  MsDataset.load('zjj1126/News_test', subset_name='default', split='train')\n",
    "# eval_dataset =  MsDataset.load('zjj1126/News_test', subset_name='default', split='test')\n",
    "# def cfg_modify_fn(cfg):\n",
    "#     cfg.train.max_epochs = max_epochs\n",
    "#     cfg.train.hooks = [{\n",
    "#             'type': 'TextLoggerHook',\n",
    "#             'interval': 100\n",
    "#         }]\n",
    "#     cfg.evaluation.metrics = [Metrics.seq_cls_metric]\n",
    "#     cfg['dataset'] = {\n",
    "#         'train': {\n",
    "#             'first_sequence': 'NewsContent',  # Adjusted to match your dataset column\n",
    "#             'label': 'label',\n",
    "#         }\n",
    "#     }\n",
    "#     return cfg\n",
    "# kwargs = dict(\n",
    "#     model=model_id,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     work_dir=WORK_DIR,\n",
    "#     cfg_modify_fn=cfg_modify_fn)\n",
    "# \n",
    "# \n",
    "# trainer = build_trainer(name='nlp-base-trainer', default_args=kwargs)\n",
    "# \n",
    "# print('===============================================================')\n",
    "# print('pre-trained model loaded, training started:')\n",
    "# print('===============================================================')\n",
    "# \n",
    "# trainer.train()\n",
    "# \n",
    "# print('===============================================================')\n",
    "# print('train success.')\n",
    "# print('===============================================================')\n",
    "# \n",
    "# for i in range(max_epochs):\n",
    "#     eval_results = trainer.evaluate(f'{WORK_DIR}/epoch_{i+1}.pth')\n",
    "#     print(f'epoch {i} evaluation result:')\n",
    "#     print(eval_results)\n",
    "# \n",
    "# \n",
    "# print('===============================================================')\n",
    "# print('evaluate success')\n",
    "# print('===============================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d883b200a4beb2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
